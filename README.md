# CodeReviewer

> **Status**: ğŸš§ Early Development - Not yet runnable

A multi-agent code review system that simulates real-world peer review discussions using heterogeneous LLM models, with support for both cloud-based and on-device (MacBook) inference.

## Overview

CodeReviewer introduces a novel approach to automated code review by orchestrating multiple LLM agents in different reviewer roles (Senior Developer, Junior Developer) that engage in multi-round discussionsâ€”just like human reviewers would. This collaborative approach aims to produce more nuanced, thorough, and reliable code reviews than single-agent systems.

### Why Multi-Agent?

Traditional automated code review tools use a single model to analyze code. CodeReviewer takes a different approach:

- **Role-Based Agents**: Each LLM assumes a specific role (Senior/Junior) with distinct perspectives and system prompts
- **Multi-Round Discussions**: Agents review each other's feedback, building consensus through iterative dialogue
- **Heterogeneous Models**: Different agents can use different models (cloud or local) optimized for their role
- **Consensus Detection**: The system identifies when agents reach agreement, mimicking real code review workflows

## Key Features (Planned)

- âœ… **Multi-Agent Pipeline**: Configurable conversation flow between Senior and Junior developer agents
- âœ… **Hybrid Inference**: Seamlessly switch between cloud APIs (OpenAI, Anthropic) and on-device models
- âœ… **On-Device First**: Optimized for Apple Silicon with quantized models (GGUF) and GPU acceleration
- âœ… **Multiple Input Sources**: Support for Git diffs, local files, and GitHub Pull Requests
- âœ… **Conversation Memory**: Intelligent context management for long discussions
- âœ… **Consensus Detection**: Automatic detection of agreement between agents
- âœ… **Streaming Output**: Real-time feedback as agents discuss code

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              UI Layer (Future: Tauri 2.0)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Multi-Agent Pipeline Coordinator           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Senior Agent â†â†’ Junior Agent             â”‚      â”‚
â”‚   â”‚ â€¢ Conversation State Machine             â”‚      â”‚
â”‚   â”‚ â€¢ Round Management (up to N rounds)      â”‚      â”‚
â”‚   â”‚ â€¢ Consensus Detection                    â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           LLM Backend Abstraction Layer             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Cloud APIs   â”‚   On-Device Inference     â”‚      â”‚
â”‚   â”‚ - OpenAI     â”‚   - Candle (GGUF)         â”‚      â”‚
â”‚   â”‚ - Anthropic  â”‚   - Metal Acceleration    â”‚      â”‚
â”‚   â”‚              â”‚   - CoreML (future)       â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Code Input Layer                       â”‚
â”‚   â€¢ Git Diffs (git2) â€¢ Files â€¢ GitHub PRs           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure (Planned)

```
code-reviewer/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs
â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ agent.rs              // Agent trait & implementation
â”‚   â”‚   â”œâ”€â”€ roles.rs              // Senior/Junior role definitions
â”‚   â”‚   â”œâ”€â”€ pipeline.rs           // Multi-round discussion coordinator
â”‚   â”‚   â””â”€â”€ consensus.rs          // Agreement detection logic
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ provider.rs           // LLMProvider trait
â”‚   â”‚   â”œâ”€â”€ cloud.rs              // Cloud backend (llm-connector)
â”‚   â”‚   â”œâ”€â”€ candle_backend.rs     // Candle on-device
â”‚   â”‚   â”œâ”€â”€ coreml_backend.rs     // CoreML (future)
â”‚   â”‚   â””â”€â”€ config.rs             // Backend configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ code_input/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ source.rs             // CodeSource trait
â”‚   â”‚   â”œâ”€â”€ git.rs                // Git diff handling (git2)
â”‚   â”‚   â”œâ”€â”€ files.rs              // File reader
â”‚   â”‚   â””â”€â”€ github.rs             // GitHub PR fetcher (octocrab)
â”‚   â”‚
â”‚   â”œâ”€â”€ conversation/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ memory.rs             // Conversation state management
â”‚   â”‚   â”œâ”€â”€ message.rs            // Message types
â”‚   â”‚   â””â”€â”€ state_machine.rs      // Discussion state transitions
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ senior_prompts.rs     // Senior developer system prompts
â”‚   â”‚   â””â”€â”€ junior_prompts.rs     // Junior developer system prompts
â”‚   â”‚
â”‚   â””â”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â””â”€â”€ commands.rs           // CLI interface
â”‚   â”‚
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚   â”‚   â””â”€â”€ mod.rs                // Contains Rust code to load, parse, and manage config
â”‚
â”œâ”€â”€ models/                       // TBD: Quantized models directory (gitignored)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.toml               // Application configuration
â””â”€â”€ tests/
    â”œâ”€â”€ integration/
    â””â”€â”€ unit/
```

## Tech Stack

| Component | Technology | Rationale |
|-----------|------------|-----------|
| **Language** | Rust 2021 | Memory safety, performance, async support |
| **Cloud APIs** | llm-connector | Unified interface for OpenAI/Anthropic |
| **On-Device** | Candle + Candle-Transformers | Pure Rust, GGUF support, Metal acceleration |
| **Git Integration** | git2 | Mature libgit2 bindings |
| **GitHub API** | octocrab | Async PR/issue management |
| **Async Runtime** | tokio | Industry standard |
| **Future UI** | Tauri 2.0 | Lightweight, native macOS integration |

## Recommended Models for On-Device

- **StarCoder2-7B** (Q5_K_M): Specialized for code understanding
- **CodeLlama-7B-Instruct** (Q5_K_M): Good instruction following
- **Qwen2.5-Coder-7B** (Q5_K_M): Latest, excellent code comprehension
- **DeepSeek-Coder-6.7B** (Q5_K_M): Strong coding capabilities

_All models will use Q5_K_M quantization for optimal size/quality balance (~1.6GB per model)_

## Roadmap

### Phase 1: Foundation
- [x] Architecture design
- [ ] Core abstractions (LLMProvider, Agent, CodeSource traits)
- [ ] Cloud API integration
- [ ] Git diff parsing
- [ ] Basic 2-agent conversation (1 round)
- [ ] CLI interface

### Phase 2: Multi-Round Pipeline
- [ ] Conversation state machine
- [ ] Memory management (sliding window)
- [ ] Multi-round discussion logic (up to 5 rounds)
- [ ] Consensus detection algorithm
- [ ] GitHub PR input support

### Phase 3: On-Device Inference
- [ ] Candle integration for GGUF models
- [ ] Model download/caching system
- [ ] Metal GPU acceleration
- [ ] Performance benchmarking

### Phase 4: CoreML Optimization
- [ ] Apple Neural Engine support
- [ ] Model conversion pipeline
- [ ] Performance tuning

### Phase 5: UI & Polish
- [ ] Tauri 2.0 desktop application
- [ ] Real-time streaming output
- [ ] Review history & export (Markdown/PDF)
- [ ] Settings UI

### Future Enhancements
- [ ] Product Manager agent (3rd role)
- [ ] Fine-tuned models on code review datasets
- [ ] GitHub App for automatic PR reviews
- [ ] Vector database integration for codebase context
- [ ] Web interface

## Performance Targets: TBD

| Metric | Cloud | Local (Q5_K_M) |
|--------|-------|----------------|
| First Token | 200-500ms | 1-3s |
| Throughput | 50-100 tok/s | 20-40 tok/s |
| Memory | <500MB | 2-3GB |
| Full Review (500 LOC) | 30-60s | 2-4min |

## Contributing

This project is in early development. Contributions, ideas, and feedback are welcome once the foundation is stable.

## License

_To be determined_

---

**Author**: [ikhyunAn](https://github.com/ikhyunAn)

**Last Updated**: October 2025